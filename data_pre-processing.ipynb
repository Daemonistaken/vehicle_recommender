{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c73e897",
   "metadata": {},
   "source": [
    "## Data analysis and Pre-processing\n",
    "- Analyze data distribution\n",
    "    - Categorical Data Distribution\n",
    "    - Numerical Data Distribution\n",
    "    - Analyze data (find correlations)\n",
    "    - Find outliers and tag them\n",
    "## Hybrid collaborative filtering\n",
    "- Feature Engineering\n",
    "- Vehicle similarity Matrix\n",
    "- Ideal Profile\n",
    "- Hybrid IBCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1821b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from mapsAPI import run_route_pipeline\n",
    "\n",
    "from logger_setup import get_file_logger\n",
    "\n",
    "logger = get_file_logger(\"data_pre-processing_\")\n",
    "\n",
    "\n",
    "#====================\n",
    "#== Configuration ==\n",
    "#====================\n",
    "# Quantity of vehicles output\n",
    "top_n = 20\n",
    "\n",
    "# Choose between using a custom route or a predefined one \n",
    "custom_route = False\n",
    "if custom_route:\n",
    "    logger.debug(f\"Using custom route\")\n",
    "    '''\n",
    "    # Example\n",
    "    run_route_pipeline(\n",
    "            origin=\"Girona, Spain\",\n",
    "            destination=\"Sevilla, Spain\",\n",
    "            waypoints=[\"Tarragona, Spain\", \"Valencia, Spain\", \"Madrid, Spain\"]\n",
    "        )\n",
    "    '''\n",
    "    # Write origin, destination and waypoints\n",
    "    route_data = run_route_pipeline(\n",
    "        origin=\"7100 Estremoz, Portugal\", \n",
    "        destination=\"7350 Elvas, Portugal\",\n",
    "        waypoints=[] # Waypoints CAN BE EMPTY, must be written as \"place 1\", \"place 2\"\n",
    "    )    \n",
    "\n",
    "else:\n",
    "    logger.debug(f\"Using predefined route\") \n",
    "    # Select predefined route data up to 51\n",
    "    route = 1 # 0..51\n",
    "    route_df = pd.read_csv('datasets/routes/route_info_data.csv')\n",
    "    route_data = route_df.iloc[route]\n",
    "    logger.debug(f\"Using route with\\n{route_data.head}\")\n",
    "\n",
    "# Fullfill preferences\n",
    "user_prefs = {\n",
    "    \"max_price_euro\": 50000, # Up to 383.318\n",
    "    \"max_car_age\": 20, # Up to 25\n",
    "    \"wanted_fuel_types\": [\"Diesel\",\"Petrol\"], #\"Diesel\", \"Petrol\", \"CNG\", \"LPG\"\n",
    "    \"wanted_transmissions\": [\"Manual\",\"Automatic\"] # \"Manual\",\"Automatic\"\n",
    "}\n",
    "\n",
    "dataset_save_path = \"datasets/proper_ibcf_results.csv\"\n",
    "df = pd.read_csv(\"datasets/vehicles/clean_car_df.csv\")\n",
    "df = df.drop(columns=\"price_lakh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc11d64",
   "metadata": {},
   "source": [
    "# Categorical Data Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f5803f",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_columns = df.select_dtypes('object')\n",
    "\n",
    "# Get numerical columns\n",
    "numerical_cols = df.select_dtypes(include=np.number).columns.drop([\"owner_number\",\"seats\"])\n",
    "# Get categorical columns\n",
    "catregorical_cols = [\"transmission\",\"owner_number\",\"seats\",\"fuel_type\",\"location\"]\n",
    "# Create subplots\n",
    "fig, axes1 = plt.subplots(2, 3, figsize=(10, 5))\n",
    "axes1 = axes1.flatten()\n",
    "\n",
    "# Plot each numerical column with histogram and KDE\n",
    "for i, col in enumerate(catregorical_cols):\n",
    "    sns.countplot(data=df,x=col, ax=axes1[i],palette='viridis',hue=col, legend=False)\n",
    "    axes1[i].set_title(col)\n",
    "    axes1[i].tick_params(axis='x', rotation=75)  # Rotate x-axis labels for better readability\n",
    "\n",
    "for i in range(len(catregorical_cols), len(axes1)):\n",
    "    axes1[i].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f72fb5c",
   "metadata": {},
   "source": [
    "# Numerical Data Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8418d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes2 = plt.subplots(3, 3, figsize=(10, 8))\n",
    "axes2 = axes2.flatten()\n",
    "\n",
    "# Plot each numerical column with histogram and KDE\n",
    "for i, col in enumerate(numerical_cols):\n",
    "    sns.histplot(data=df, x=col, stat='percent',kde=True, ax=axes2[i], bins=30)\n",
    "    axes2[i].set_title(col)\n",
    "    axes2[i].tick_params(axis='x', rotation=75)  # Rotate x-axis labels for better readability\n",
    "\n",
    "# Hide unused subplots\n",
    "for j in range(len(numerical_cols), len(axes2)):\n",
    "    axes2[j].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c972ba60",
   "metadata": {},
   "source": [
    "## Analyze outliers\n",
    "\n",
    "Since we are working on a dataset with cars we can't really point out and delete outliers since there are many types of cars with different specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0055ef23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding outliers using IQR method for numerical columns\n",
    "def find_outliers_iqr(df):\n",
    "    outlier_index = dict()\n",
    "    info = []\n",
    "    numeric_columns = df.select_dtypes(include=np.number).drop(columns=[\"owner_number\",\"seats\"])\n",
    "    for col in numeric_columns:\n",
    "        Q1 = df[col].quantile(0.25) # Quantile lower\n",
    "        Q3 = df[col].quantile(0.75) # Quantile upper\n",
    "        IQR = Q3 - Q1 # Interquartile range\n",
    "        lower_quantile = Q1 - (1.5 * IQR) # Lower bound\n",
    "        upper_quantile = Q3 + (1.5 * IQR) # Upper bound\n",
    "\n",
    "        outliers = df[(df[col]<lower_quantile) | (df[col]> upper_quantile)] # Get data from below lower bound and above upper bound\n",
    "        outlier_index[col] = outliers.index.tolist() # Get the index of the outliers\n",
    "\n",
    "        info.append({\n",
    "            'feature': col,              \n",
    "            'Q1': Q1,\n",
    "            'Q3': Q3,\n",
    "            'IQR': IQR,\n",
    "            'Lower Bound': lower_quantile,\n",
    "            'Upper Bound': upper_quantile        \n",
    "        })\n",
    "    return pd.DataFrame(info), outlier_index\n",
    "\n",
    "def plot_outliers(df, outlier_index):\n",
    "    # Create a single figure with 7 rows and 2 columns\n",
    "    fig, axes = plt.subplots(7, 2, figsize=(15, 25))\n",
    "    \n",
    "    # Get the list of columns with outliers\n",
    "    columns = list(outlier_index.keys())\n",
    "    \n",
    "    for i, (col, indices) in enumerate(outlier_index.items()):\n",
    "        # First column - Boxplot\n",
    "        sns.boxplot(x=df[col], ax=axes[i, 0])\n",
    "        axes[i, 0].set_title(f'Boxplot de {col}', fontsize=14)\n",
    "        axes[i, 0].set_xlabel(col)\n",
    "        \n",
    "        # Second column - Scatterplot with outliers\n",
    "        # Paint all the points in the scatterplot\n",
    "        sns.scatterplot(x=df.index, y=df[col], label='Datos', alpha=0.6, ax=axes[i, 1])\n",
    "        # Paint the outliers in red over the scatterplot from before\n",
    "        sns.scatterplot(x=indices, y=df.loc[indices, col], color='red', label='Outliers', s=60, ax=axes[i, 1])\n",
    "        axes[i, 1].set_title(f'Posibles outliers en {col}', fontsize=14)\n",
    "        axes[i, 1].set_xlabel('Indice')\n",
    "        axes[i, 1].set_ylabel(col)\n",
    "        #axes[i, 1].legend()\n",
    "        axes[i, 1].legend(bbox_to_anchor=(1.05, 1), loc='upper left',markerfirst=False)\n",
    "    \n",
    "    # Hide any unused subplots if there are fewer than 7 variables\n",
    "    for j in range(len(columns), 7):\n",
    "        axes[j, 0].set_visible(False)\n",
    "        axes[j, 1].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function\n",
    "summary, outlier_index = find_outliers_iqr(df)\n",
    "print(summary)\n",
    "plot_outliers(df, outlier_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583db518",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_quartile_tags(df, outlier_index, column_mappings=None):\n",
    "    \"\"\"\n",
    "    Creates binary columns for quartile-based outlier tagging\n",
    "    \n",
    "    Parameters:\n",
    "    df: DataFrame with the data\n",
    "    outlier_index: Dictionary with column names as keys and outlier indices as values\n",
    "    column_mappings: Dictionary mapping column names to their low/high labels\n",
    "                    Example: {'age': ('Old_car', 'New_car'), 'price': ('Cheap_car', 'Expensive_car')}\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame with new binary columns added\n",
    "    \"\"\"\n",
    "    \n",
    "    # Default mappings if none provided\n",
    "    if column_mappings is None:\n",
    "        column_mappings = {}\n",
    "    \n",
    "    # Create a copy of the dataframe to avoid modifying the original\n",
    "    df_tagged = df.copy()\n",
    "    \n",
    "    for col in outlier_index.keys():\n",
    "        # Calculate quartiles\n",
    "        Q1 = df[col].quantile(0.25) # Quantile lower\n",
    "        Q3 = df[col].quantile(0.75) # Quantile upper\n",
    "        IQR = Q3 - Q1 # Interquartile range\n",
    "        q25 = Q1 - (1.5 * IQR) # Lower bound\n",
    "        q75 = Q3 + (1.5 * IQR) # Upper bound\n",
    "\n",
    "        #q25 = df[col].quantile(0.25)\n",
    "        #q75 = df[col].quantile(0.75)\n",
    "        \n",
    "        # Get custom labels or create default ones\n",
    "        if col in column_mappings:\n",
    "            low_label, high_label = column_mappings[col]\n",
    "        else:\n",
    "            # Create default labels based on column name\n",
    "            low_label = f\"low_{col}\"\n",
    "            high_label = f\"high_{col}\"\n",
    "        \n",
    "        # Create binary columns\n",
    "        # 1 for values below Q25, 0 otherwise\n",
    "        df_tagged[low_label] = (df[col] < q25).astype(int)\n",
    "        \n",
    "        # 1 for values above Q75, 0 otherwise  \n",
    "        df_tagged[high_label] = (df[col] > q75).astype(int)\n",
    "        \n",
    "        print(f\"Created columns for {col}:\")\n",
    "        print(f\"  - {low_label}: {df_tagged[low_label].sum()} records tagged\")\n",
    "        print(f\"  - {high_label}: {df_tagged[high_label].sum()} records tagged\")\n",
    "        print()\n",
    "    \n",
    "    return df_tagged\n",
    "\n",
    "# Example usage:\n",
    "# Define custom mappings for your columns\n",
    "column_mappings = {\n",
    "    'age': ('Old_car', 'New_car'),\n",
    "    'price': ('Cheap_car', 'Expensive_car'),\n",
    "    'mileage': ('Low_mileage', 'High_mileage'),\n",
    "    'engine_size': ('Small_engine', 'Large_engine')\n",
    "    # Add more mappings as needed for your specific columns\n",
    "}\n",
    "\n",
    "# Create the tagged dataframe\n",
    "df_tagged = create_quartile_tags(df, outlier_index, column_mappings)\n",
    "\n",
    "# Display the new columns\n",
    "print(\"New binary columns created:\")\n",
    "new_columns = [col for col in df_tagged.columns if col not in df.columns]\n",
    "print(new_columns)\n",
    "\n",
    "# Show some examples\n",
    "print(\"\\nSample of tagged data:\")\n",
    "#print(df_tagged[new_columns].head(10))\n",
    "df_tagged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d79fb45",
   "metadata": {},
   "source": [
    "# Hybrid IBCF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411a0754",
   "metadata": {},
   "source": [
    "## Correlation matrix between both vehicles and route features\n",
    "\n",
    "As we can see vehicle features does not correlate with route features so we'll have to do feature engineering to make the features meaningful for the cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87ecac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "route_df = pd.read_csv('datasets/routes/route_info_data.csv')\n",
    "vehicle_num_df = df.select_dtypes(include=np.number)\n",
    "route_num_df = route_df.select_dtypes(include=np.number)\n",
    "\n",
    "# OPTIONAL: Repeat route_df to match number of vehicle rows (for correlation)\n",
    "# For meaningful correlation, both DataFrames should have same number of rows\n",
    "route_repeated = pd.concat([route_num_df]*len(df), ignore_index=True)\n",
    "\n",
    "# Combine them\n",
    "combined_df = pd.concat([vehicle_num_df.reset_index(drop=True), route_repeated], axis=1)\n",
    "\n",
    "# Now compute correlation\n",
    "correlation_matrix = combined_df.corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Matriz de correlacion de las variables numericas de vehiculos y rutas')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e8424c",
   "metadata": {},
   "source": [
    "## Feature engineering \n",
    "Each car specification (like age, horsepower, engine size, price) originally has a totally different range.\n",
    "To make comparisons fair, we scale all of them between 0 and 1 using normalization.\n",
    "Then we create three combined scores that better reflect what really matters in a vehicle:\n",
    "- Performance = mostly power + some engine size\n",
    "- Economy = mostly fuel efficiency + some affordability\n",
    "- Overall quality = a mix of reliability, performance, and economy\n",
    "## Vehicle Similarity Matrix\n",
    "Then we compare every car to every other car using cosine similarity, which basically checks how \"aligned\" two cars are across all features. This gives us a score for each pair of cars: how similar their capabilities are.\n",
    "## Ideal vehicle profile\n",
    "We create a made-up car profile that would be perfect for the route, for example, a steep long trip would call for high power, fuel efficiency, and reliability.\n",
    "We then compare that ideal car to all real cars using cosine similarity again, giving each real car a direct match score.\n",
    "## Hybrid IBCF\n",
    "For each real car A, we check how similar it is to every other car (based on the earlier car-to-car matrix), Then weight those similarites by how well those other cars match the ideal profile and this gives us an IBCF score.\n",
    "## Hybrid vehicles ranking\n",
    "Then we normalize both scores (ideal similarity and ibcf score) and average them 50/50 to get a hybrid score, which balances how well a car matches the route and how similar it is to cars that do\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13ad101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_ibcf(route_data,vehicle_df,top_n_vehicles):\n",
    "    #=========================\n",
    "    #== Feature Engineering ==\n",
    "    #=========================\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    vehicle_features = vehicle_df.copy()\n",
    "\n",
    "    logger.debug(\"Creating new feature vectors\")\n",
    "    # Create new feature vectors to enrich dataset information\n",
    "    score_map = {\n",
    "        'reliability_score': (vehicle_features[['car_age']].max() - vehicle_features[['car_age']]).values,\n",
    "        'fuel_efficiency_score': vehicle_features[['mileage_kmpl']].values,\n",
    "        'power_score': vehicle_features[['power_bhp']].values,\n",
    "        'size_score': vehicle_features[['engine_cc']].values,\n",
    "        'affordability_score': (vehicle_features[['price_euro']].max() - vehicle_features[['price_euro']]).values\n",
    "    }\n",
    "\n",
    "    logger.debug(f\"Normalizing newly created features\")\n",
    "    # Normalize the values for each feature\n",
    "    for key, value in score_map.items():\n",
    "        vehicle_features[key] = scaler.fit_transform(value).flatten()\n",
    "    \n",
    "    logger.debug(f\"Setting up weights:\")\n",
    "    # Create composite features to make cosine-similarity and IBCF more meaningful\n",
    "    weight = {\n",
    "        #Performance score\n",
    "        'power_score': 0.6,\n",
    "        'size_score': 0.4,\n",
    "        #Economy score\n",
    "        'fuel_efficiency_score':0.7,\n",
    "        'affordability_score':0.3,\n",
    "        #Overall quality score\n",
    "        'reliability_score':0.4,\n",
    "        'performance_score':0.3,\n",
    "        'economy_score':0.3\n",
    "    }\n",
    "    logger.debug(f\"Creating new composite features\")\n",
    "    # performance_score = (w1) * power_score + (w1) * size_score\n",
    "    vehicle_features['performance_score'] = (\n",
    "        weight['power_score'] * vehicle_features['power_score'] + \n",
    "        weight['size_score'] * vehicle_features['size_score']\n",
    "    )\n",
    "    # economy_score = ((w1) * fuel_efficiency_score) + ((w2) * affordability_score)\n",
    "    vehicle_features['economy_score'] = (\n",
    "        weight['fuel_efficiency_score'] * vehicle_features['fuel_efficiency_score']\n",
    "        +weight['affordability_score']* vehicle_features['affordability_score']\n",
    "    )\n",
    "    # overall_quality_score = ((w1) * reliability_score) + ((w2) * performance_score) + ((w3) * economy_score)\n",
    "    vehicle_features['overall_quality_score'] = (\n",
    "        weight['reliability_score'] * vehicle_features['reliability_score']\n",
    "        + weight['performance_score'] * vehicle_features['performance_score']\n",
    "        + weight['economy_score'] * vehicle_features['economy_score']\n",
    "    )\n",
    "    # Group all new features\n",
    "    unified_features = [\n",
    "        'reliability_score', 'fuel_efficiency_score', 'power_score', \n",
    "        'size_score', 'affordability_score', 'performance_score', \n",
    "        'economy_score', 'overall_quality_score'\n",
    "    ]\n",
    "\n",
    "    #===============================\n",
    "    #== Vehicle Similarity Matrix ==\n",
    "    #===============================\n",
    "    logger.debug(f\"Computing newly created features into a cosine similarity\")\n",
    "    vehicle_matrix = vehicle_features[unified_features].values\n",
    "    vehicle_similarity_matrix = cosine_similarity(vehicle_matrix)\n",
    "    np.fill_diagonal(vehicle_similarity_matrix, 0) # Avoid self-similarity\n",
    "    logger.debug(f\"New vehicle matrix generated\")\n",
    "    #===========================\n",
    "    #== Ideal Vehicle Profile ==\n",
    "    #===========================\n",
    "    logger.debug(f\"Extracting route features\")\n",
    "    # Route features\n",
    "    distance_km = route_data['distance_km']\n",
    "    elevation_gain = route_data['elevation_gain_m']\n",
    "    avg_slope = route_data['avg_slope']\n",
    "\n",
    "    # Rule based preference\n",
    "    # Each metric decides how important the feature is for the ideal vehicle profile\n",
    "    # Example:\n",
    "    # Fuel_efficiency_score: The higher the distance the more importance it has over the ideal profile\n",
    "    #\n",
    "    logger.debug(f\"Creating new ideal vehicle profile based on route features\")\n",
    "    ideal_vehicle = np.zeros(len(unified_features)) # List filled with zeroes\n",
    "    #reliability_score\n",
    "    ideal_vehicle[0] = 0.9 if distance_km > 200 else 0.7 if distance_km > 100 else 0.5 \n",
    "    #fuel_efficiency_score\n",
    "    ideal_vehicle[1] = 0.8 if distance_km > 150 else 0.6\n",
    "    #power_score\n",
    "    ideal_vehicle[2] = 0.9 if elevation_gain > 500 or avg_slope > 4 else 0.7 if elevation_gain > 200 or avg_slope > 2 else 0.5\n",
    "    #size_score\n",
    "    ideal_vehicle[3] = ideal_vehicle[2] * 0.8\n",
    "    #affordability_score\n",
    "    ideal_vehicle[4] = 0.4 if distance_km > 200 else 0.7\n",
    "    #performance_score\n",
    "    ideal_vehicle[5] = 0.6 * ideal_vehicle[2] + 0.4 * ideal_vehicle[3]\n",
    "    #economy_score\n",
    "    ideal_vehicle[6] = 0.7 * ideal_vehicle[1] + 0.3 * ideal_vehicle[4]\n",
    "    #overall_quality_score\n",
    "    ideal_vehicle[7] = 0.4 * ideal_vehicle[0] + 0.3 * ideal_vehicle[5] + 0.3 * ideal_vehicle[6]\n",
    "    ideal_profile_weights = pd.DataFrame(ideal_vehicle.reshape(1, -1), columns=unified_features)\n",
    "    ideal_profile_weights.T.rename(columns={0: 'Ideal Profile values'})\n",
    "\n",
    "    #=================\n",
    "    #== Hybrid IBCF == \n",
    "    #=================\n",
    "    logger.debug(f\"Computing cosine similarity with new ideal vehicle profile and previous vehicle matrix\")\n",
    "    ideal_similarity = cosine_similarity([ideal_vehicle], vehicle_matrix)[0]\n",
    "    logger.debug(f\"Computing similar ibcf technique to improve vehicle relations\")\n",
    "    ibcf_scores_list = []\n",
    "    for vehicle_index in range(len(vehicle_features)):\n",
    "        # similarity of vehicle idx to all other vehicles\n",
    "        sim_to_others = vehicle_similarity_matrix[vehicle_index]\n",
    "        # weighted sum: how much these other vehicles match the ideal\n",
    "        weighted_match = np.sum(sim_to_others * ideal_similarity)\n",
    "        ibcf_scores_list.append(weighted_match)\n",
    "    # convert to numpy array for consistency\n",
    "    ibcf_scores = np.array(ibcf_scores_list)\n",
    "\n",
    "    #======================\n",
    "    #== Ranking Vehicles ==\n",
    "    #======================\n",
    "    logger.debug(f\"Normalizing ideal_similarity matrix and ibcf matrix \")\n",
    "    # Normalization of ideal and ibcf similarity scores\n",
    "    normalization_ideal = (ideal_similarity - ideal_similarity.min()) / (ideal_similarity.max() - ideal_similarity.min())\n",
    "    normalization_ibcf = (ibcf_scores - ibcf_scores.min()) / (ibcf_scores.max() - ibcf_scores.min())\n",
    "    \n",
    "    # Alpha: How much you trust the ideal similarity match\n",
    "    # Beta: How much you trust the IBCF similarity\n",
    "    \n",
    "    alpha, beta = 0.5, 0.5\n",
    "    #logger(f\"Defining final hybrid score with {alpha} ideal_vehicle weight and {beta} of ibcf weight\")\n",
    "    hybrid_score = alpha * normalization_ideal + beta * normalization_ibcf\n",
    "\n",
    "    vehicle_features['hybrid_score'] = hybrid_score\n",
    "    vehicle_features['direct_similarity'] = ideal_similarity\n",
    "    vehicle_features['ibcf_score'] = ibcf_scores\n",
    "    # Could sort values between hybrid, direct and ibcf score\n",
    "    score_type = 'hybrid_score'\n",
    "    logger.debug(f\"Ranking final vehicles with {score_type}\")\n",
    "    ibcf_results = vehicle_features.sort_values(score_type, ascending=False).head(top_n_vehicles)\n",
    "    ibcf_results.to_csv(dataset_save_path, index=False)\n",
    "    logger.info(f\"ibcf_results saved to {dataset_save_path}\")\n",
    "    logger.debug(f\"Finalizing vehicle ranking\")\n",
    "    return ideal_similarity,ibcf_scores,vehicle_features,ibcf_results,ideal_profile_weights,unified_features\n",
    "\n",
    "vehicle_df = df_tagged.copy()\n",
    "# Applying the filters to the dataset\n",
    "logger.debug(f\"Before applying filter: {vehicle_df.shape}\")\n",
    "vehicle_df = vehicle_df[\n",
    "    (vehicle_df[\"price_euro\"] <= user_prefs[\"max_price_euro\"]) &\n",
    "    (vehicle_df[\"car_age\"] <= user_prefs[\"max_car_age\"]) &\n",
    "    (vehicle_df[\"fuel_type\"].isin(user_prefs[\"wanted_fuel_types\"])) &\n",
    "    (vehicle_df[\"transmission\"].isin(user_prefs[\"wanted_transmissions\"]))\n",
    "].reset_index(drop=True)\n",
    "logger.debug(f\"After applying filter: {vehicle_df.shape}\")\n",
    "\n",
    "if vehicle_df.empty:\n",
    "    logger.warning(\"No vehicles match the user's preferences.\")    \n",
    "else: \n",
    "    normalization_ideal,normalization_ibcf,ranked_vehicles_df,top_n_ranked_vehicles_df,ideal_profile,ideal_profile_features = hybrid_ibcf(route_data,vehicle_df,top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca29b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "sns.histplot(ranked_vehicles_df['hybrid_score'], bins=50, kde=True, color='blue')\n",
    "plt.title('Vehicle similarity to the ideal profile')\n",
    "plt.xlabel('Hybrid score from ideal profile cosine similarity and IBCF')\n",
    "plt.ylabel('Quantity of vehicles')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "sns.barplot(ideal_profile)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Feature weight value on ideal_similarity score\")\n",
    "plt.show()\n",
    "\n",
    "top_n_ranked_vehicles_df.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
